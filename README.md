# PrivateGPT (Local Ollama Frontend)

Small frontend-first chat app that uses a local Ollama instance for LLM generations.

## Requirements

- Ollama installed and running locally (default HTTP API at `http://localhost:11434`).
- Node.js (for helper scripts and optional static server).

## Quick setup

1. Generate the models list (preferred — uses local `ollama` CLI):

```bash
npm run ollama:models
```

This writes `public/models.json` which the frontend prefers when populating the model selector.

2. Serve the `public/` folder locally (static server):

```bash
npx serve public
# or any static file server you prefer
```

3. Open the served site in your browser (e.g., http://localhost:5000) and interact with the chat.

## How it works

- The frontend loads the available models from `public/models.json` (generated by step 1).
- If `public/models.json` is missing, the UI falls back to calling `http://localhost:11434/api/models` directly.
- Chat generation uses Ollama's streaming `/api/generate` endpoint; the client includes robust parsing to handle NDJSON or concatenated JSON chunks.

## CORS note

Browsers may block direct requests to Ollama's HTTP API due to CORS. Preferred workflow:

- Use `npm run ollama:models` to produce `public/models.json` so model discovery doesn't require the browser calling Ollama CLI endpoints.
- If streaming generation from the browser is blocked by CORS, either enable CORS in your Ollama setup (if configurable) or run a tiny local proxy (not provided by default).

## Commands

- `npm run ollama:models` — generate `public/models.json` from local Ollama.
- `npx serve public` — serve the frontend for local testing.

## Troubleshooting

- If you see JSON fragments in the chat UI, ensure Ollama is running locally and that the frontend's `public/modules/chat.js` contains the latest stream-parsing fixes.
- If the model list doesn't populate, run the `ollama:models` script and reload the page.

## License

Private to your project.
